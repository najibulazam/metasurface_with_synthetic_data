


# Standard libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import pickle
from tqdm import tqdm

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split

# Sklearn for metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Set style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11

# Check GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üñ•Ô∏è  Device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)

print("\n‚úÖ All libraries imported successfully!")





class SyntheticMetasurfaceData:
    """Generate synthetic metasurface data using analytical phase model"""
    
    def __init__(self, wavelength=632, n_substrate=1.5, n_pillar=3.5):
        self.wavelength = wavelength  # nm
        self.n_substrate = n_substrate
        self.n_pillar = n_pillar
        
    def compute_phase(self, width, height):
        """
        Analytical phase model for rectangular nanopillar
        
        Args:
            width: Pillar width in nm (50-300)
            height: Pillar height in nm (200-600)
            
        Returns:
            phase: Phase shift in radians (0 to 2œÄ)
        """
        # Normalize parameters
        w_norm = width / self.wavelength
        h_norm = height / self.wavelength
        
        # Component 1: Propagation phase (dominant component)
        phase_prop = 2 * np.pi * (self.n_pillar - 1) * h_norm
        
        # Component 2: Geometric resonance (width-dependent)
        resonance_factor = 1 + 0.5 * np.sin(2 * np.pi * w_norm * 3)
        
        # Component 3: Coupling (width-height interaction)
        coupling = 0.3 * np.sin(w_norm * h_norm * 10)
        
        # Combine and wrap to [0, 2œÄ]
        phase = (phase_prop * resonance_factor + coupling) % (2 * np.pi)
        
        return phase
    
    def compute_transmission(self, width, height):
        """Analytical transmission model"""
        w_norm = width / self.wavelength
        h_norm = height / self.wavelength
        
        # Base transmission (decreases with height)
        T_base = np.exp(-0.5 * h_norm)
        
        # Width-dependent modulation
        T_modulation = 0.2 * np.cos(2 * np.pi * w_norm * 2)
        
        transmission = np.clip(T_base + T_modulation, 0.1, 0.95)
        
        return transmission
    
    def generate_dataset(self, n_samples=5000, noise_level=0.02):
        """
        Generate dataset with random sampling
        
        Args:
            n_samples: Number of samples
            noise_level: Noise standard deviation
            
        Returns:
            DataFrame with width_nm, height_nm, phase_rad, transmission
        """
        print(f"Generating {n_samples} samples...")
        
        # Random sampling in parameter space
        widths = np.random.uniform(50, 300, n_samples)
        heights = np.random.uniform(200, 600, n_samples)
        
        data = []
        for w, h in tqdm(zip(widths, heights), total=n_samples, desc="Computing"):
            phase = self.compute_phase(w, h)
            transmission = self.compute_transmission(w, h)
            
            data.append({
                'width_nm': w,
                'height_nm': h,
                'phase_rad': phase,
                'transmission': transmission
            })
        
        df = pd.DataFrame(data)
        
        # Add realistic measurement noise
        df['phase_rad'] += np.random.normal(0, noise_level, len(df))
        df['phase_rad'] = df['phase_rad'] % (2 * np.pi)  # Wrap to [0, 2œÄ]
        
        df['transmission'] += np.random.normal(0, noise_level * 0.5, len(df))
        df['transmission'] = np.clip(df['transmission'], 0, 1)
        
        print(f"‚úÖ Generated {len(df)} samples")
        return df

# Initialize generator
generator = SyntheticMetasurfaceData(wavelength=632)

# Generate dataset (THIS IS FAST - NO FDTD!)
df = generator.generate_dataset(n_samples=5000, noise_level=0.02)

print("\nüìä Dataset Statistics:")
print(df.describe())


# Visualize generated dataset
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Phase vs Width
ax1 = axes[0, 0]
scatter1 = ax1.scatter(df['width_nm'], df['phase_rad'], 
                       c=df['height_nm'], s=10, alpha=0.6, cmap='viridis')
ax1.set_xlabel('Width (nm)', fontsize=13, fontweight='bold')
ax1.set_ylabel('Phase (rad)', fontsize=13, fontweight='bold')
ax1.set_title('Phase vs Width (colored by Height)', fontsize=14, fontweight='bold')
plt.colorbar(scatter1, ax=ax1, label='Height (nm)')
ax1.grid(True, alpha=0.3)

# 2. Phase vs Height
ax2 = axes[0, 1]
scatter2 = ax2.scatter(df['height_nm'], df['phase_rad'],
                       c=df['width_nm'], s=10, alpha=0.6, cmap='plasma')
ax2.set_xlabel('Height (nm)', fontsize=13, fontweight='bold')
ax2.set_ylabel('Phase (rad)', fontsize=13, fontweight='bold')
ax2.set_title('Phase vs Height (colored by Width)', fontsize=14, fontweight='bold')
plt.colorbar(scatter2, ax=ax2, label='Width (nm)')
ax2.grid(True, alpha=0.3)

# 3. 2D Phase map
ax3 = axes[0, 2]
width_grid = np.linspace(50, 300, 100)
height_grid = np.linspace(200, 600, 100)
W, H = np.meshgrid(width_grid, height_grid)
Z = np.zeros_like(W)
for i in range(W.shape[0]):
    for j in range(W.shape[1]):
        Z[i, j] = generator.compute_phase(W[i, j], H[i, j])

contour = ax3.contourf(W, H, Z, levels=20, cmap='twilight')
ax3.set_xlabel('Width (nm)', fontsize=13, fontweight='bold')
ax3.set_ylabel('Height (nm)', fontsize=13, fontweight='bold')
ax3.set_title('Phase Landscape (Forward Model)', fontsize=14, fontweight='bold')
plt.colorbar(contour, ax=ax3, label='Phase (rad)')

# 4. Width distribution
ax4 = axes[1, 0]
ax4.hist(df['width_nm'], bins=50, alpha=0.7, color='blue', edgecolor='black')
ax4.set_xlabel('Width (nm)', fontsize=13, fontweight='bold')
ax4.set_ylabel('Count', fontsize=13, fontweight='bold')
ax4.set_title('Width Distribution', fontsize=14, fontweight='bold')
ax4.grid(True, alpha=0.3)

# 5. Height distribution
ax5 = axes[1, 1]
ax5.hist(df['height_nm'], bins=50, alpha=0.7, color='green', edgecolor='black')
ax5.set_xlabel('Height (nm)', fontsize=13, fontweight='bold')
ax5.set_ylabel('Count', fontsize=13, fontweight='bold')
ax5.set_title('Height Distribution', fontsize=14, fontweight='bold')
ax5.grid(True, alpha=0.3)

# 6. Phase distribution
ax6 = axes[1, 2]
ax6.hist(df['phase_rad'], bins=50, alpha=0.7, color='red', edgecolor='black')
ax6.set_xlabel('Phase (rad)', fontsize=13, fontweight='bold')
ax6.set_ylabel('Count', fontsize=13, fontweight='bold')
ax6.set_title('Phase Distribution', fontsize=14, fontweight='bold')
ax6.axvline(np.pi, color='black', linestyle='--', linewidth=2, label='œÄ')
ax6.legend()
ax6.grid(True, alpha=0.3)

plt.suptitle('Synthetic Dataset Visualization', fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

# Check phase coverage
phase_coverage = (df['phase_rad'].max() - df['phase_rad'].min()) / (2 * np.pi)
print(f"\nüìà Phase coverage: {phase_coverage*100:.1f}% of 2œÄ range")
print(f"   Min phase: {df['phase_rad'].min():.3f} rad ({np.rad2deg(df['phase_rad'].min()):.1f}¬∞)")
print(f"   Max phase: {df['phase_rad'].max():.3f} rad ({np.rad2deg(df['phase_rad'].max()):.1f}¬∞)")





# For inverse design: X = (phase, transmission), y = (width, height)
# Using 2 inputs makes the problem well-constrained!
X = df[['phase_rad', 'transmission']].values  # 2 features now
y = df[['width_nm', 'height_nm']].values

print(f"Input shape: {X.shape}")
print(f"Output shape: {y.shape}")

# Normalize inputs
scaler_X = StandardScaler()
X_normalized = scaler_X.fit_transform(X)

# Normalize outputs (CRITICAL for training stability!)
scaler_y = StandardScaler()
y_normalized = scaler_y.fit_transform(y)

print(f"\nNormalized input mean: {X_normalized.mean(axis=0)}")
print(f"Normalized input std: {X_normalized.std(axis=0)}")
print(f"Normalized output mean: {y_normalized.mean(axis=0)}")
print(f"Normalized output std: {y_normalized.std(axis=0)}")

# PyTorch Dataset
class MetasurfaceDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Create dataset
dataset = MetasurfaceDataset(X_normalized, y_normalized)

# Split data: 70% train, 15% val, 15% test
total_size = len(dataset)
train_size = int(0.70 * total_size)
val_size = int(0.15 * total_size)
test_size = total_size - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(
    dataset, 
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)
)

# Create data loaders
BATCH_SIZE = 64
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"\nüì¶ Data Loaders:")
print(f"   Train samples: {len(train_dataset)}")
print(f"   Validation samples: {len(val_dataset)}")
print(f"   Test samples: {len(test_dataset)}")
print(f"   Batch size: {BATCH_SIZE}")

# Save scalers for later use
scalers = {
    'X_scaler': scaler_X,
    'y_scaler': scaler_y
}





class InverseDesignMLP(nn.Module):
    """Multi-Layer Perceptron for inverse design"""
    
    def __init__(self, input_dim=2, hidden_dims=[128, 256, 256, 128], output_dim=2, dropout=0.2):
        super(InverseDesignMLP, self).__init__()
        
        # Build layers dynamically
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(hidden_dim))
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        
        # Output layer
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        
        # Initialize weights (Xavier initialization)
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        return self.network(x)
    
    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

# Create model with 2 input features (phase + transmission)
model = InverseDesignMLP(
    input_dim=2,  # Changed from 1 to 2!
    hidden_dims=[128, 256, 256, 128],
    output_dim=2,
    dropout=0.2
).to(device)

print(f"üß† Model: InverseDesignMLP")
print(f"   Input features: 2 (phase + transmission)")
print(f"   Output features: 2 (width + height)")
print(f"   Parameters: {model.count_parameters():,}")
print(f"   Device: {device}")
print(f"\nüìê Architecture:")
print(model)





# Training configuration
NUM_EPOCHS = 300
LEARNING_RATE = 0.001
PATIENCE = 50  # Early stopping

# Setup optimizer and loss
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.MSELoss()
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=20
)

# Training history
history = {
    'train_loss': [],
    'val_loss': [],
    'learning_rate': []
}

best_val_loss = float('inf')
patience_counter = 0
best_model_state = None  # Initialize to avoid NameError

print("="*60)
print("üöÄ TRAINING STARTED")
print("="*60)
print(f"Epochs: {NUM_EPOCHS}")
print(f"Learning rate: {LEARNING_RATE}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Early stopping patience: {PATIENCE}")
print(f"LR scheduler: ReduceLROnPlateau (patience=20, factor=0.5)")
print("="*60 + "\n")

start_time = time.time()

# Training loop
for epoch in range(NUM_EPOCHS):
    # TRAIN
    model.train()
    train_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        pred = model(batch_X)
        loss = criterion(pred, batch_y)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    # VALIDATE
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X = batch_X.to(device)
            batch_y = batch_y.to(device)
            
            pred = model(batch_X)
            loss = criterion(pred, batch_y)
            
            val_loss += loss.item()
    
    val_loss /= len(val_loader)
    
    # Store old learning rate (for manual verbose output)
    old_lr = optimizer.param_groups[0]['lr']
    
    # Update learning rate
    scheduler.step(val_loss)
    current_lr = optimizer.param_groups[0]['lr']
    
    # Manual verbose output for LR changes
    if current_lr < old_lr:
        print(f"\nüìâ Reducing learning rate: {old_lr:.6f} ‚Üí {current_lr:.6f}\n")
    
    # Save history
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['learning_rate'].append(current_lr)
    
    # Print progress
    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(f"Epoch {epoch+1:3d}/{NUM_EPOCHS} | "
              f"Train: {train_loss:.6f} | "
              f"Val: {val_loss:.6f} | "
              f"LR: {current_lr:.6f}")
    
    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict()
        patience_counter = 0
    else:
        patience_counter += 1
    
    # Early stopping
    if patience_counter >= PATIENCE:
        print(f"\n‚ö†Ô∏è Early stopping at epoch {epoch+1}")
        break

elapsed_time = time.time() - start_time

print("\n" + "="*60)
print("‚úÖ TRAINING COMPLETE!")
print("="*60)
print(f"Time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)")
print(f"Best validation loss: {best_val_loss:.6f}")
print(f"Total epochs: {len(history['train_loss'])}")
print("="*60)

# Load best model (only if we have one)
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    print("‚úÖ Loaded best model weights")
else:
    print("‚ö†Ô∏è  No best model saved, using final epoch weights")


# Plot training history
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

epochs = range(1, len(history['train_loss']) + 1)

# Loss curves
ax1 = axes[0]
ax1.plot(epochs, history['train_loss'], label='Train Loss', linewidth=2, marker='o', markersize=3)
ax1.plot(epochs, history['val_loss'], label='Validation Loss', linewidth=2, marker='s', markersize=3)
ax1.set_xlabel('Epoch', fontsize=13, fontweight='bold')
ax1.set_ylabel('MSE Loss', fontsize=13, fontweight='bold')
ax1.set_title('Training Progress', fontsize=14, fontweight='bold')
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)
ax1.set_yscale('log')

# Learning rate
ax2 = axes[1]
ax2.plot(epochs, history['learning_rate'], color='green', linewidth=2)
ax2.set_xlabel('Epoch', fontsize=13, fontweight='bold')
ax2.set_ylabel('Learning Rate', fontsize=13, fontweight='bold')
ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.set_yscale('log')

plt.suptitle('Training Convergence', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print(f"\nüìâ Final train loss: {history['train_loss'][-1]:.6f}")
print(f"üìâ Final validation loss: {history['val_loss'][-1]:.6f}")





# Evaluate on test set
model.eval()

all_X = []
all_y_true = []
all_y_pred = []

with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)
        
        pred = model(batch_X)
        
        all_X.append(batch_X.cpu().numpy())
        all_y_true.append(batch_y.cpu().numpy())
        all_y_pred.append(pred.cpu().numpy())

# Concatenate batches
X_test = np.concatenate(all_X, axis=0)
y_true = np.concatenate(all_y_true, axis=0)
y_pred = np.concatenate(all_y_pred, axis=0)

# Denormalize predictions
X_test_denorm = scalers['X_scaler'].inverse_transform(X_test)  # Back to original scale
y_true_denorm = scalers['y_scaler'].inverse_transform(y_true)
y_pred_denorm = scalers['y_scaler'].inverse_transform(y_pred)

# Calculate metrics
mae_width = mean_absolute_error(y_true_denorm[:, 0], y_pred_denorm[:, 0])
mae_height = mean_absolute_error(y_true_denorm[:, 1], y_pred_denorm[:, 1])

rmse_width = np.sqrt(mean_squared_error(y_true_denorm[:, 0], y_pred_denorm[:, 0]))
rmse_height = np.sqrt(mean_squared_error(y_true_denorm[:, 1], y_pred_denorm[:, 1]))

r2_width = r2_score(y_true_denorm[:, 0], y_pred_denorm[:, 0])
r2_height = r2_score(y_true_denorm[:, 1], y_pred_denorm[:, 1])

print("="*60)
print("üìä EVALUATION METRICS (Test Set)")
print("="*60)
print("\nMean Absolute Error (MAE):")
print(f"  Width:  {mae_width:.2f} nm")
print(f"  Height: {mae_height:.2f} nm")
print(f"  Average: {(mae_width + mae_height)/2:.2f} nm")

print("\nRoot Mean Squared Error (RMSE):")
print(f"  Width:  {rmse_width:.2f} nm")
print(f"  Height: {rmse_height:.2f} nm")

print("\nR¬≤ Score (1.0 = perfect):")
print(f"  Width:  {r2_width:.4f}")
print(f"  Height: {r2_height:.4f}")

print("\n" + "="*60)

# Detailed error statistics
error_width = y_pred_denorm[:, 0] - y_true_denorm[:, 0]
error_height = y_pred_denorm[:, 1] - y_true_denorm[:, 1]

print("\nüìà Error Statistics:")
print(f"Width errors - Mean: {error_width.mean():.2f} nm, Std: {error_width.std():.2f} nm")
print(f"Height errors - Mean: {error_height.mean():.2f} nm, Std: {error_height.std():.2f} nm")


# Comprehensive prediction visualization
fig = plt.figure(figsize=(18, 12))

# 1. Width predictions
ax1 = plt.subplot(2, 3, 1)
ax1.scatter(y_true_denorm[:, 0], y_pred_denorm[:, 0], 
            alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
min_w = min(y_true_denorm[:, 0].min(), y_pred_denorm[:, 0].min())
max_w = max(y_true_denorm[:, 0].max(), y_pred_denorm[:, 0].max())
ax1.plot([min_w, max_w], [min_w, max_w], 'r--', linewidth=3, label='Perfect Prediction')
ax1.set_xlabel('True Width (nm)', fontsize=13, fontweight='bold')
ax1.set_ylabel('Predicted Width (nm)', fontsize=13, fontweight='bold')
ax1.set_title(f'Width Prediction\nMAE={mae_width:.2f}nm, R¬≤={r2_width:.3f}', 
              fontsize=14, fontweight='bold')
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)

# 2. Height predictions
ax2 = plt.subplot(2, 3, 2)
ax2.scatter(y_true_denorm[:, 1], y_pred_denorm[:, 1],
            alpha=0.5, s=30, edgecolors='k', linewidth=0.5, color='green')
min_h = min(y_true_denorm[:, 1].min(), y_pred_denorm[:, 1].min())
max_h = max(y_true_denorm[:, 1].max(), y_pred_denorm[:, 1].max())
ax2.plot([min_h, max_h], [min_h, max_h], 'r--', linewidth=3, label='Perfect Prediction')
ax2.set_xlabel('True Height (nm)', fontsize=13, fontweight='bold')
ax2.set_ylabel('Predicted Height (nm)', fontsize=13, fontweight='bold')
ax2.set_title(f'Height Prediction\nMAE={mae_height:.2f}nm, R¬≤={r2_height:.3f}',
              fontsize=14, fontweight='bold')
ax2.legend(fontsize=11)
ax2.grid(True, alpha=0.3)

# 3. Error vs phase (width)
ax3 = plt.subplot(2, 3, 3)
ax3.scatter(X_test_denorm[:, 0], error_width, alpha=0.5, s=30)  # Phase is column 0
ax3.axhline(0, color='r', linestyle='--', linewidth=3)
ax3.set_xlabel('Phase (rad)', fontsize=13, fontweight='bold')
ax3.set_ylabel('Width Error (nm)', fontsize=13, fontweight='bold')
ax3.set_title('Width Error vs Phase', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3)

# 4. Error vs transmission (height)
ax4 = plt.subplot(2, 3, 4)
ax4.scatter(X_test_denorm[:, 1], error_height, alpha=0.5, s=30, color='green')  # Transmission is column 1
ax4.axhline(0, color='r', linestyle='--', linewidth=3)
ax4.set_xlabel('Transmission', fontsize=13, fontweight='bold')
ax4.set_ylabel('Height Error (nm)', fontsize=13, fontweight='bold')
ax4.set_title('Height Error vs Transmission', fontsize=14, fontweight='bold')
ax4.grid(True, alpha=0.3)

# 5. Error distribution (width)
ax5 = plt.subplot(2, 3, 5)
ax5.hist(error_width, bins=50, alpha=0.7, edgecolor='black', color='blue')
ax5.axvline(0, color='r', linestyle='--', linewidth=3)
ax5.set_xlabel('Width Error (nm)', fontsize=13, fontweight='bold')
ax5.set_ylabel('Count', fontsize=13, fontweight='bold')
ax5.set_title(f'Width Error Distribution\nStd={error_width.std():.2f}nm',
              fontsize=14, fontweight='bold')
ax5.grid(True, alpha=0.3)

# 6. Error distribution (height)
ax6 = plt.subplot(2, 3, 6)
ax6.hist(error_height, bins=50, alpha=0.7, color='green', edgecolor='black')
ax6.axvline(0, color='r', linestyle='--', linewidth=3)
ax6.set_xlabel('Height Error (nm)', fontsize=13, fontweight='bold')
ax6.set_ylabel('Count', fontsize=13, fontweight='bold')
ax6.set_title(f'Height Error Distribution\nStd={error_height.std():.2f}nm',
              fontsize=14, fontweight='bold')
ax6.grid(True, alpha=0.3)

plt.suptitle('üéØ Inverse Design Model Evaluation Results (2 Inputs)', fontsize=17, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()





# Test specific phase values with transmission
test_phases_rad = np.linspace(0, 2*np.pi, 9)  # 0¬∞, 45¬∞, 90¬∞, 135¬∞, 180¬∞, 225¬∞, 270¬∞, 315¬∞, 360¬∞
test_phases_deg = np.rad2deg(test_phases_rad)

# Use median transmission from dataset for demo
median_transmission = df['transmission'].median()
test_transmissions = np.full_like(test_phases_rad, median_transmission)

# Create 2-input pairs: (phase, transmission)
X_demo = np.column_stack([test_phases_rad, test_transmissions])
X_demo_normalized = scalers['X_scaler'].transform(X_demo)
X_demo_tensor = torch.FloatTensor(X_demo_normalized).to(device)

# Predict
model.eval()
with torch.no_grad():
    y_demo = model(X_demo_tensor).cpu().numpy()

# Denormalize predictions
y_demo_denorm = scalers['y_scaler'].inverse_transform(y_demo)

# Create results table
results = pd.DataFrame({
    'Phase (rad)': test_phases_rad,
    'Phase (deg)': test_phases_deg,
    'Transmission': test_transmissions,
    'Predicted Width (nm)': y_demo_denorm[:, 0],
    'Predicted Height (nm)': y_demo_denorm[:, 1]
})

print("="*80)
print("üéØ INVERSE DESIGN PREDICTIONS: (PHASE, TRANSMISSION) ‚Üí GEOMETRY")
print("="*80)
print(f"\nGiven desired phase and transmission={median_transmission:.3f},")
print("predict nanopillar geometry to fabricate:\n")
print(results.to_string(index=False))
print("\n" + "="*80)

# Visualize predictions
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Width vs Phase
ax1.plot(test_phases_deg, y_demo_denorm[:, 0], 
         marker='o', markersize=12, linewidth=3, color='blue', label='Predicted Width')
ax1.set_xlabel('Desired Phase (degrees)', fontsize=14, fontweight='bold')
ax1.set_ylabel('Required Width (nm)', fontsize=14, fontweight='bold')
ax1.set_title(f'Inverse Design: Phase ‚Üí Width (T={median_transmission:.3f})', fontsize=15, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend(fontsize=12)

# Height vs Phase
ax2.plot(test_phases_deg, y_demo_denorm[:, 1], 
         marker='s', markersize=12, linewidth=3, color='green', label='Predicted Height')
ax2.set_xlabel('Desired Phase (degrees)', fontsize=14, fontweight='bold')
ax2.set_ylabel('Required Height (nm)', fontsize=14, fontweight='bold')
ax2.set_title(f'Inverse Design: Phase ‚Üí Height (T={median_transmission:.3f})', fontsize=15, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend(fontsize=12)

plt.suptitle('üéØ Design Recommendations (2-Input Model)', fontsize=17, fontweight='bold')
plt.tight_layout()
plt.show()





# Create output directory
output_dir = Path('inverse_design_outputs')
output_dir.mkdir(exist_ok=True)

# 1. Save trained model
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'history': history,
    'best_val_loss': best_val_loss,
    'metrics': {
        'mae_width': mae_width,
        'mae_height': mae_height,
        'r2_width': r2_width,
        'r2_height': r2_height
    }
}, output_dir / 'trained_model.pth')

print("‚úÖ Saved: trained_model.pth")

# 2. Save scalers
with open(output_dir / 'scalers.pkl', 'wb') as f:
    pickle.dump(scalers, f)

print("‚úÖ Saved: scalers.pkl")

# 3. Save dataset
df.to_csv(output_dir / 'synthetic_dataset.csv', index=False)
print("‚úÖ Saved: synthetic_dataset.csv")

# 4. Save demo predictions
results.to_csv(output_dir / 'demo_predictions.csv', index=False)
print("‚úÖ Saved: demo_predictions.csv")

# 5. Save test set predictions
test_predictions = pd.DataFrame({
    'phase_rad': X_test_denorm[:, 0],
    'transmission': X_test_denorm[:, 1],
    'width_true': y_true_denorm[:, 0],
    'height_true': y_true_denorm[:, 1],
    'width_pred': y_pred_denorm[:, 0],
    'height_pred': y_pred_denorm[:, 1],
    'width_error': error_width,
    'height_error': error_height
})
test_predictions.to_csv(output_dir / 'test_predictions.csv', index=False)
print("‚úÖ Saved: test_predictions.csv")

# 6. Save metrics summary
metrics_summary = {
    'Training': {
        'Total epochs': len(history['train_loss']),
        'Training time (min)': elapsed_time / 60,
        'Best validation loss': best_val_loss,
        'Final train loss': history['train_loss'][-1],
        'Final val loss': history['val_loss'][-1]
    },
    'Test Set Performance': {
        'MAE Width (nm)': mae_width,
        'MAE Height (nm)': mae_height,
        'MAE Average (nm)': (mae_width + mae_height) / 2,
        'RMSE Width (nm)': rmse_width,
        'RMSE Height (nm)': rmse_height,
        'R¬≤ Width': r2_width,
        'R¬≤ Height': r2_height
    },
    'Model': {
        'Architecture': 'MLP',
        'Parameters': model.count_parameters(),
        'Input dim': 2,
        'Output dim': 2,
        'Hidden layers': [128, 256, 256, 128]
    },
    'Dataset': {
        'Total samples': len(df),
        'Train samples': len(train_dataset),
        'Val samples': len(val_dataset),
        'Test samples': len(test_dataset),
        'Noise level': 0.02
    }
}

import json
with open(output_dir / 'metrics_summary.json', 'w') as f:
    json.dump(metrics_summary, f, indent=4)

print("‚úÖ Saved: metrics_summary.json")

print("\n" + "="*60)
print(f"üìÅ All outputs saved to: {output_dir}")
print("="*60)

# List all files
print("\nSaved files:")
for file in output_dir.iterdir():
    size = file.stat().st_size / 1024  # KB
    print(f"  - {file.name} ({size:.1f} KB)")





from kaggle_secrets import UserSecretsClient
import os

# get token from kaggle secrets
user_secrets = UserSecretsClient()
token = user_secrets.get_secret("GITHUB_TOKEN")

username = "najibulazam"
repo = "metasurface_with_synthetic_data"
email = "najibul.me@gmail.com"

get_ipython().run_line_magic("cd", " /kaggle/working")

# init git
get_ipython().getoutput("git init")
get_ipython().getoutput("git config user.name "{username}"")
get_ipython().getoutput("git config user.email "{email}"")

# remove old remote
get_ipython().getoutput("git remote remove origin 2>/dev/null")

# add github remote
get_ipython().getoutput("git remote add origin https://{username}:{token}@github.com/{username}/{repo}.git")

# ignore junk
with open(".gitignore","w") as f:
    f.write("""
.ipynb_checkpoints/
__pycache__/
*.log
*.tmp
kaggle.json
""")

# add all files
get_ipython().getoutput("git add .")

# commit
get_ipython().getoutput("git commit -m "Auto backup from Kaggle notebook" || echo "No changes"")

# push
get_ipython().getoutput("git branch -M main")
get_ipython().getoutput("git push -u origin main --force")


import os
print("TOKEN FOUND:", "GITHUB_TOKEN" in os.environ)




